{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier\n",
    "This notebook loads and the resnet50 classififer and performs feature extraction using the generated data from the first notebook.\n",
    "To predict only pedestrians, the output classes are reduced from 1000 (original ImageNet output size) to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./data/classifierImages/preparedForTrainTestVal\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training \n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "# when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transorms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True,) for x in ['train', 'val']}\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_images': 0, 'pos_images': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a short look how the images are labeled ...\n",
    "image_datasets['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the requires_grad attribute to False if only the last layer should be updated (feature extraction)\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to C:\\Users\\peter/.cache\\torch\\checkpoints\\resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce74e94a7454c858e1fc79c6b87b60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of output layers was reduced from 1000 to 2.\n",
      "Since resnet50 uses \"AdaptiveAvgPool2d(output_size=(1, 1))\" the size of the input image can be any size ...\n"
     ]
    }
   ],
   "source": [
    "# instantiate pretrained model \n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "# remove original output layer and replace it with 2 dimensinal layer to detect, if an image patch contains a pedestrian or not\n",
    "number_output_features = model_ft.fc.out_features\n",
    "number_input_features  = model_ft.fc.in_features\n",
    "\n",
    "model_ft.fc = nn.Linear(number_input_features, num_classes)\n",
    "\n",
    "print(f'The number of output layers was reduced from {number_output_features} to 2.')\n",
    "\n",
    "# renset50 uses an adaptive avg pooling layer at the end\n",
    "# this means the size of the input images does not matter, since the planes are always flattern to the same size\n",
    "print(f'Since resnet50 uses \"{model_ft.avgpool}\" the size of the input image can be any size ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    #   In train mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                   \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.4139 Acc: 0.8182\n",
      "val Loss: 0.2128 Acc: 0.9524\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.2357 Acc: 0.9130\n",
      "val Loss: 0.1090 Acc: 0.9702\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.1964 Acc: 0.9289\n",
      "val Loss: 0.0909 Acc: 0.9762\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.1933 Acc: 0.9269\n",
      "val Loss: 0.1014 Acc: 0.9643\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.1629 Acc: 0.9526\n",
      "val Loss: 0.1097 Acc: 0.9643\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.9289\n",
      "val Loss: 0.0713 Acc: 0.9762\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1563 Acc: 0.9447\n",
      "val Loss: 0.0801 Acc: 0.9762\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.2333 Acc: 0.9111\n",
      "val Loss: 0.0941 Acc: 0.9702\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1441 Acc: 0.9368\n",
      "val Loss: 0.0701 Acc: 0.9821\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.9387\n",
      "val Loss: 0.0551 Acc: 0.9762\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1785 Acc: 0.9427\n",
      "val Loss: 0.0569 Acc: 0.9821\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.1546 Acc: 0.9387\n",
      "val Loss: 0.0546 Acc: 0.9821\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1357 Acc: 0.9447\n",
      "val Loss: 0.0598 Acc: 0.9881\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.9407\n",
      "val Loss: 0.0550 Acc: 0.9702\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1541 Acc: 0.9447\n",
      "val Loss: 0.0528 Acc: 0.9762\n",
      "\n",
      "Training complete in 111m 5s\n",
      "Best val Acc: 0.988095\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save current model\n",
    "model_dir = './saved_models/'\n",
    "model_name = 'pedestrianClassifier15Epochs.pt'\n",
    "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "torch.save(model_ft.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders for test set ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Datasets and Dataloaders for test set ...\")\n",
    "# Create test dataset\n",
    "image_dataset_test = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "# Create test dataloader\n",
    "dataloader_test = torch.utils.data.DataLoader(image_dataset_test, batch_size=1, shuffle=True)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores from test data\n",
    "def evaluate(dataloader_test, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader_test:          \n",
    "            images, labels = data\n",
    "            images =torch.unsqueeze(images,0)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += 1\n",
    "            if (predicted == labels):\n",
    "                correct+=1\n",
    "    print(f'Accuracy of the network on the {total} test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 172 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(image_dataset_test, model_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
